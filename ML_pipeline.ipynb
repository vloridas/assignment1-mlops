{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3bf0613-d6ec-4cf4-87e5-062fd3bd3a82",
   "metadata": {},
   "source": [
    "### Installation\n",
    "Install the packages required for executing this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69f1d825-84cc-43ac-9fe2-f204d77f0962",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kfp>2 in /opt/conda/lib/python3.10/site-packages (2.14.4)\n",
      "Collecting kfp>2\n",
      "  Downloading kfp-2.14.6-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting google-cloud-pipeline-components>2\n",
      "  Downloading google_cloud_pipeline_components-2.21.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: google-cloud-aiplatform in /opt/conda/lib/python3.10/site-packages (1.120.0)\n",
      "Collecting google-cloud-aiplatform\n",
      "  Downloading google_cloud_aiplatform-1.122.0-py2.py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: click==8.1.8 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (8.1.8)\n",
      "Requirement already satisfied: click-option-group==0.5.7 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (0.5.7)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (0.17.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (2.26.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.1 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (2.41.1)\n",
      "Requirement already satisfied: google-cloud-storage<4,>=2.2.1 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (2.19.0)\n",
      "Collecting kfp-pipeline-spec<3,>=2.14.3 (from kfp>2)\n",
      "  Downloading kfp_pipeline_spec-2.14.6-py3-none-any.whl.metadata (433 bytes)\n",
      "Requirement already satisfied: kfp-server-api<3,>=2.14.3 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (2.14.3)\n",
      "Requirement already satisfied: kubernetes<31,>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (30.1.0)\n",
      "Requirement already satisfied: protobuf<7.0,>=6.31.1 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (6.31.1)\n",
      "Requirement already satisfied: PyYAML<7,>=5.3 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (6.0.3)\n",
      "Requirement already satisfied: requests-toolbelt<2,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (1.0.0)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (0.9.0)\n",
      "Requirement already satisfied: urllib3<3.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (2.5.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>2) (1.70.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>2) (1.26.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>2) (2.32.5)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp>2) (6.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp>2) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp>2) (4.9.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<4,>=2.2.1->kfp>2) (2.4.3)\n",
      "Requirement already satisfied: google-resumable-media>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<4,>=2.2.1->kfp>2) (2.7.2)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<4,>=2.2.1->kfp>2) (1.7.1)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.10/site-packages (from kfp-server-api<3,>=2.14.3->kfp>2) (1.17.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from kfp-server-api<3,>=2.14.3->kfp>2) (2025.10.5)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from kfp-server-api<3,>=2.14.3->kfp>2) (2.9.0.post0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes<31,>=8.0.0->kfp>2) (1.9.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.10/site-packages (from kubernetes<31,>=8.0.0->kfp>2) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /opt/conda/lib/python3.10/site-packages (from kubernetes<31,>=8.0.0->kfp>2) (3.3.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>2) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>2) (3.10)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from rsa<5,>=3.1.4->google-auth<3,>=1.6.1->kfp>2) (0.6.1)\n",
      "Requirement already satisfied: Jinja2<4,>=3.1.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-pipeline-components>2) (3.1.6)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (25.0)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.38.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.14.2)\n",
      "Requirement already satisfied: shapely<3.0.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.1.2)\n",
      "Requirement already satisfied: google-genai<2.0.0,>=1.37.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.42.0)\n",
      "Requirement already satisfied: pydantic<3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.12.0)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (4.15.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.75.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.75.1)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0,>=1.3.3->google-cloud-aiplatform) (0.14.2)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (4.11.0)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /opt/conda/lib/python3.10/site-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (0.28.1)\n",
      "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /opt/conda/lib/python3.10/site-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (9.1.2)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (15.0.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (1.3.1)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2<4,>=3.1.2->google-cloud-pipeline-components>2) (3.0.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (2.41.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (0.4.2)\n",
      "Requirement already satisfied: numpy>=1.21 in /opt/conda/lib/python3.10/site-packages (from shapely<3.0.0->google-cloud-aiplatform) (1.26.4)\n",
      "Downloading kfp-2.14.6-py3-none-any.whl (374 kB)\n",
      "Downloading kfp_pipeline_spec-2.14.6-py3-none-any.whl (9.6 kB)\n",
      "Downloading google_cloud_pipeline_components-2.21.0-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_cloud_aiplatform-1.122.0-py2.py3-none-any.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m202.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: kfp-pipeline-spec, kfp, google-cloud-aiplatform, google-cloud-pipeline-components\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/4\u001b[0m [kfp]\u001b[33m  WARNING: The scripts dsl-compile and kfp are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [google-cloud-aiplatform]\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [google-cloud-pipeline-components]peline-components]\n",
      "\u001b[1A\u001b[2KSuccessfully installed google-cloud-aiplatform-1.122.0 google-cloud-pipeline-components-2.21.0 kfp-2.14.6 kfp-pipeline-spec-2.14.6\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install the packages\n",
    "! pip3 install --user --no-cache-dir --upgrade \"kfp>2\" \"google-cloud-pipeline-components>2\" \\\n",
    "                                        google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bc6a21-604f-4a52-b904-e3bb18a61b2f",
   "metadata": {},
   "source": [
    "## Restart the kernel\n",
    "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52dad0c4-c173-46b8-bf99-d6e8efc35316",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2207b06-771f-4dbb-a713-90c50745c0ea",
   "metadata": {},
   "source": [
    "## Check the versions of the packages you installed. The KFP SDK version should be >2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5b60838-e5a2-41cd-ae93-43925343fba5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 2.14.6\n",
      "google-cloud-aiplatform==1.122.0\n",
      "google_cloud_pipeline_components version: 2.21.0\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! pip3 freeze | grep aiplatform\n",
    "! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f0bcff2-3ffb-4e51-b852-511cb10ad0f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "import typing\n",
    "from typing import Dict\n",
    "from typing import NamedTuple\n",
    "from kfp import dsl\n",
    "from kfp.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        ClassificationMetrics,\n",
    "                        component, \n",
    "                        OutputPath, \n",
    "                        InputPath)\n",
    "import google.cloud.aiplatform as aip\n",
    "from google_cloud_pipeline_components.types import artifact_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01afffb0-449b-4669-807a-793f526277fe",
   "metadata": {},
   "source": [
    "#### Project and Pipeline Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abf6aad4-f675-47aa-820b-14daa796d89f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#The Google Cloud project that this pipeline runs in.\n",
    "PROJECT_ID = \"atomic-wall-471907-n1\"\n",
    "# The region that this pipeline runs in\n",
    "REGION = \"us-central1\"\n",
    "# Specify a Cloud Storage URI that your pipelines service account can access. The artifacts of your pipeline runs are stored within the pipeline root.\n",
    "PIPELINE_ROOT = \"gs://ass1_temp_bucket/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f8a366-76c3-4f17-b760-f012dfacaf02",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed1a411c-2f31-45ae-89f8-f394c3157fab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\",\"tensorflow\",\"scikit-learn\", \"fsspec\",\"gcsfs\"],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def train_model(data_bucket:str, output_model: Output[Model]):\n",
    "    \"\"\"\n",
    "    Function takes data file from the data bucket and trains a simple MLP model on it.\n",
    "    \"\"\"\n",
    "    import random\n",
    "    import pandas as pd\n",
    "    from tensorflow import keras\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    random.seed(67)\n",
    "\n",
    "    # 1. Load dataset from data bucket\n",
    "    #url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\"\n",
    "    cols = [\"variance\", \"skewness\", \"curtosis\", \"entropy\", \"class\"]\n",
    "    #df = pd.read_csv(url, header=None, names=cols)\n",
    "    \n",
    "    df = pd.read_csv(f\"gs://{data_bucket}/data_banknote_authentication.txt\", header=None, names=cols)\n",
    "\n",
    "    X = df[[\"variance\", \"skewness\", \"curtosis\", \"entropy\"]].values\n",
    "    y = df[\"class\"].values\n",
    "\n",
    "    # 2. Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # 3. Normalize inside the model (so we don’t need a separate scaler)\n",
    "    normalizer = keras.layers.Normalization()\n",
    "    normalizer.adapt(X_train)\n",
    "\n",
    "    # 4. Build simple MLP model\n",
    "    model = keras.Sequential([\n",
    "        normalizer,\n",
    "        keras.layers.Dense(8, activation=\"relu\"),\n",
    "        keras.layers.Dense(4, activation=\"relu\"),\n",
    "        keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    # 5. Train model\n",
    "    model.fit(X_train, y_train, epochs=15, batch_size=8, validation_split=0.2, verbose=1)\n",
    "\n",
    "    # 6. Evaluate model\n",
    "    loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\" Test accuracy: {acc:.3f}\")\n",
    "\n",
    "    # 7. Save model in the same folder\n",
    "    model.save(\"model.keras\")\n",
    "    print(\" Model saved as model.keras\")\n",
    "    \n",
    "    # 8. Add metadata\n",
    "    metadata = {\n",
    "        \"accuracy\": acc,\n",
    "        \"algo\": \"MLP\",\n",
    "        \"file_type\": \".keras\"\n",
    "    }\n",
    "        \n",
    "    # 9. Attach metadata to Vertex artifact (for pipelines)\n",
    "    output_model.metadata.update(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b9e9d5e-2b30-4727-97c0-69b0f334628c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:00:13.799 - INFO - Executing task \u001b[96m'train-model'\u001b[0m\n",
      "13:00:13.801 - INFO - Streamed logs:\n",
      "\n",
      "    Found image 'python:3.10.7-slim'\n",
      "\n",
      "    WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "    WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "    \n",
      "    [notice] A new release of pip available: 22.2.2 -> 25.3\n",
      "    [notice] To update, run: pip install --upgrade pip\n",
      "    [KFP Executor 2025-10-29 13:01:16,880 INFO]: Looking for component `train_model` in --component_module_path `/tmp/tmp.SZsU6BZTM1/ephemeral_component.py`\n",
      "    [KFP Executor 2025-10-29 13:01:16,880 INFO]: Loading KFP component \"train_model\" from /tmp/tmp.SZsU6BZTM1/ephemeral_component.py (directory \"/tmp/tmp.SZsU6BZTM1\" and module name \"ephemeral_component\")\n",
      "    [KFP Executor 2025-10-29 13:01:16,881 INFO]: Got executor_input:\n",
      "    {\n",
      "        \"inputs\": {\n",
      "            \"parameterValues\": {\n",
      "                \"data_bucket\": \"ass1_data_bucket\"\n",
      "            }\n",
      "        },\n",
      "        \"outputs\": {\n",
      "            \"artifacts\": {\n",
      "                \"output_model\": {\n",
      "                    \"artifacts\": [\n",
      "                        {\n",
      "                            \"name\": \"output_model\",\n",
      "                            \"type\": {\n",
      "                                \"schemaTitle\": \"system.Model\",\n",
      "                                \"schemaVersion\": \"0.0.1\"\n",
      "                            },\n",
      "                            \"uri\": \"/home/jupyter/assignment1-mlops/local_outputs/train-model-2025-10-29-13-00-13-798771/train-model/output_model\",\n",
      "                            \"metadata\": {}\n",
      "                        }\n",
      "                    ]\n",
      "                }\n",
      "            },\n",
      "            \"outputFile\": \"/home/jupyter/assignment1-mlops/local_outputs/train-model-2025-10-29-13-00-13-798771/train-model/executor_output.json\"\n",
      "        }\n",
      "    }\n",
      "    2025-10-29 13:01:17.495055: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "    2025-10-29 13:01:17.545655: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "    To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "    2025-10-29 13:01:19.350508: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "    2025-10-29 13:01:20.456293: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "    Epoch 1/15\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6784 - loss: 0.6222 - val_accuracy: 0.8091 - val_loss: 0.5082\n",
      "    Epoch 2/15\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8883 - loss: 0.4286 - val_accuracy: 0.9773 - val_loss: 0.3482\n",
      "    Epoch 3/15\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9567 - loss: 0.2939 - val_accuracy: 0.9818 - val_loss: 0.2271\n",
      "    Epoch 4/15\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9715 - loss: 0.1954 - val_accuracy: 0.9864 - val_loss: 0.1490\n",
      "    Epoch 5/15\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9783 - loss: 0.1338 - val_accuracy: 0.9864 - val_loss: 0.1003\n",
      "    Epoch 6/15\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9806 - loss: 0.0962 - val_accuracy: 0.9864 - val_loss: 0.0723\n",
      "    Epoch 7/15\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9818 - loss: 0.0739 - val_accuracy: 0.9909 - val_loss: 0.0545\n",
      "    Epoch 8/15\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9829 - loss: 0.0600 - val_accuracy: 0.9909 - val_loss: 0.0433\n",
      "    Epoch 9/15\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9829 - loss: 0.0506 - val_accuracy: 0.9909 - val_loss: 0.0358\n",
      "    Epoch 10/15\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9829 - loss: 0.0442 - val_accuracy: 0.9909 - val_loss: 0.0306\n",
      "    Epoch 11/15\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9829 - loss: 0.0391 - val_accuracy: 0.9909 - val_loss: 0.0266\n",
      "    Epoch 12/15\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9875 - loss: 0.0349 - val_accuracy: 0.9909 - val_loss: 0.0236\n",
      "    Epoch 13/15\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9875 - loss: 0.0317 - val_accuracy: 0.9909 - val_loss: 0.0211\n",
      "    Epoch 14/15\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9875 - loss: 0.0287 - val_accuracy: 1.0000 - val_loss: 0.0186\n",
      "    Epoch 15/15\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9875 - loss: 0.0257 - val_accuracy: 1.0000 - val_loss: 0.0168\n",
      "     Test accuracy: 0.993\n",
      "     Model saved as model.keras\n",
      "    [KFP Executor 2025-10-29 13:01:26,116 INFO]: Wrote executor output file to /home/jupyter/assignment1-mlops/local_outputs/train-model-2025-10-29-13-00-13-798771/train-model/executor_output.json.\n",
      "13:01:28.644 - INFO - Task \u001b[96m'train-model'\u001b[0m finished with status \u001b[92mSUCCESS\u001b[0m\n",
      "13:01:28.645 - INFO - Task \u001b[96m'train-model'\u001b[0m outputs:\n",
      "    output_model: Model( name='output_model',\n",
      "                         uri='/home/jupyter/assignment1-mlops/local_outputs/train-model-2025-10-29-13-00-13-798771/train-model/output_model',\n",
      "                         metadata={'file_type': '.keras', 'algo': 'MLP', 'accuracy': 0.9927272796630859} )\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<kfp.dsl.pipeline_task.PipelineTask at 0x7f5aee4aafb0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick test\n",
    "from kfp import local\n",
    "local.init(runner=local.DockerRunner())\n",
    "train_model(\n",
    "    data_bucket = 'ass1_data_bucket'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1e16e7-962a-46c3-8957-009267d1256c",
   "metadata": {},
   "source": [
    "### Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "54b7759b-d6bb-437c-86a4-d817187ed834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"google-cloud-stroage\"],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def compare_model(new_model: Input[Model], model_bucket_metadata: str):\n",
    "    \"\"\"\n",
    "    Function compares local model to existing one in model bucket\n",
    "    :returns: \"NEW\" if accuracy of the new model is better, \"EXISTING\" if the old one is better.\n",
    "    \"\"\"\n",
    "    import json, tempfile, os\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    # Get new model accuracy\n",
    "    new_accuracy = float(new_model.metadata.get(\"accuracy\", 0))\n",
    "\n",
    "    # Get old model accuracy\n",
    "    bucket_name, blob_path = model_bucket_metadata.replace(\"gs://\", \"\").split(\"/\", 1)\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_path)\n",
    "    \n",
    "    # Temporarily download the older model from the model bucket\n",
    "    tmp = tempfile.NamedTemporaryFile(delete=False)\n",
    "    blob.download_to_filename(tmp.name)\n",
    "    with open(tmp.name, \"r\") as f:\n",
    "        old_metadata = json.load(f)\n",
    "        \n",
    "    old_accuracy = float(old_metadata[\"accuracy\"])\n",
    "    \n",
    "    os.remove(tmp.name)\n",
    "    # Check whether the new model outperforms the old one\n",
    "    decision = \"NEW\" if new_accuracy > old_accuracy else \"EXISTING\"\n",
    "    return decision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbccdc87-7aab-46f5-a4e5-6b24a8ad8060",
   "metadata": {},
   "source": [
    "### Upload Model and Metrics to Google Bucket "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9344f481-88ca-4220-a0af-3a055c61455b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"google-cloud-storage\"],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def upload_model_to_gcs(project_id: str, model_repo: str):\n",
    "    '''upload model to gsc'''\n",
    "    from google.cloud import storage\n",
    "    from urllib.parse import urlparse\n",
    "    import os\n",
    "    \n",
    "    # Parse bucket and prefix\n",
    "    parsed = urlparse(model_repo)\n",
    "    bucket_name = parsed.netloc\n",
    "    prefix = parsed.path.lstrip(\"/\")\n",
    "\n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.bucket(bucket_name)\n",
    "\n",
    "    # --- Upload model.keras ---\n",
    "    model_path = \"model.keras\"\n",
    "    model_blob_name = os.path.join(prefix, os.path.basename(model_path))\n",
    "    bucket.blob(model_blob_name).upload_from_filename(model_path)\n",
    "\n",
    "    # --- Upload model_metadata.json ---\n",
    "    metadata_path = \"model_metadata.json\"\n",
    "    metadata_blob_name = os.path.join(prefix, os.path.basename(metadata_path))\n",
    "    bucket.blob(metadata_blob_name).upload_from_filename(metadata_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166590b3-f788-4e4c-8e31-fb981da56966",
   "metadata": {},
   "source": [
    "#### Define the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a96b6ae0-234b-4883-ae95-8599689a5e07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'decision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Define the workflow of the pipeline.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;129;43m@kfp\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdsl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbanknote-authentication-training-pipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;21;43mpipeline\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mproject_id\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_bucket\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_repo\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_bucket_metadata\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;43;03m    Building the pipeline\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;43;03m    \"\"\"\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# New stuff\u001b[39;49;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/kfp/dsl/pipeline_context.py:71\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(func, name, description, pipeline_root, display_name, pipeline_config)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pipeline_root:\n\u001b[1;32m     69\u001b[0m     func\u001b[38;5;241m.\u001b[39mpipeline_root \u001b[38;5;241m=\u001b[39m pipeline_root\n\u001b[0;32m---> 71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomponent_factory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_graph_component_from_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplay_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/kfp/dsl/component_factory.py:712\u001b[0m, in \u001b[0;36mcreate_graph_component_from_func\u001b[0;34m(func, name, description, display_name, pipeline_config)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Implementation for the @pipeline decorator.\u001b[39;00m\n\u001b[1;32m    702\u001b[0m \n\u001b[1;32m    703\u001b[0m \u001b[38;5;124;03mThe decorator is defined under pipeline_context.py. See the\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;124;03mdecorator for the canonical documentation for this function.\u001b[39;00m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    707\u001b[0m component_spec \u001b[38;5;241m=\u001b[39m extract_component_interface(\n\u001b[1;32m    708\u001b[0m     func,\n\u001b[1;32m    709\u001b[0m     description\u001b[38;5;241m=\u001b[39mdescription,\n\u001b[1;32m    710\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    711\u001b[0m )\n\u001b[0;32m--> 712\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_component\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGraphComponent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomponent_spec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomponent_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplay_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/kfp/dsl/graph_component.py:61\u001b[0m, in \u001b[0;36mGraphComponent.__init__\u001b[0;34m(self, component_spec, pipeline_func, display_name, pipeline_config)\u001b[0m\n\u001b[1;32m     52\u001b[0m     args_list\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     53\u001b[0m         pipeline_channel\u001b[38;5;241m.\u001b[39mcreate_pipeline_channel(\n\u001b[1;32m     54\u001b[0m             name\u001b[38;5;241m=\u001b[39marg_name,\n\u001b[1;32m     55\u001b[0m             channel_type\u001b[38;5;241m=\u001b[39minput_spec\u001b[38;5;241m.\u001b[39mtype,\n\u001b[1;32m     56\u001b[0m             is_artifact_list\u001b[38;5;241m=\u001b[39minput_spec\u001b[38;5;241m.\u001b[39mis_artifact_list,\n\u001b[1;32m     57\u001b[0m         ))\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pipeline_context\u001b[38;5;241m.\u001b[39mPipeline(\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponent_spec\u001b[38;5;241m.\u001b[39mname) \u001b[38;5;28;01mas\u001b[39;00m dsl_pipeline:\n\u001b[0;32m---> 61\u001b[0m     pipeline_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dsl_pipeline\u001b[38;5;241m.\u001b[39mtasks:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTask is missing from pipeline.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[38], line 18\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(project_id, data_bucket, model_repo, model_bucket_metadata)\u001b[0m\n\u001b[1;32m      9\u001b[0m training_mlp_job_run_op \u001b[38;5;241m=\u001b[39m train_model(\n\u001b[1;32m     10\u001b[0m     data_bucket \u001b[38;5;241m=\u001b[39m data_bucket\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m compare_model_job_run_op \u001b[38;5;241m=\u001b[39m compare_model(\n\u001b[1;32m     14\u001b[0m     new_model \u001b[38;5;241m=\u001b[39m training_mlp_job_run_op\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_model\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     15\u001b[0m     model_bucket_metadata \u001b[38;5;241m=\u001b[39m model_bucket_metadata\n\u001b[1;32m     16\u001b[0m )\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dsl\u001b[38;5;241m.\u001b[39mIf(\u001b[43mcompare_model_job_run_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdecision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNEW\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     19\u001b[0m     upload_model_to_gcs(\n\u001b[1;32m     20\u001b[0m         project_id \u001b[38;5;241m=\u001b[39m project_id,\n\u001b[1;32m     21\u001b[0m         model_repo \u001b[38;5;241m=\u001b[39m model_repo\n\u001b[1;32m     22\u001b[0m     )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m# Old stuff\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m    ).after(predict_lr_job_run_op) \u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'decision'"
     ]
    }
   ],
   "source": [
    "# Define the workflow of the pipeline.\n",
    "@kfp.dsl.pipeline(\n",
    "    name=\"banknote-authentication-training-pipeline\")\n",
    "def pipeline(project_id: str, data_bucket: str, model_repo: str, model_bucket_metadata: str):\n",
    "    \"\"\"\n",
    "    Building the pipeline\n",
    "    \"\"\"\n",
    "    # New stuff\n",
    "    training_mlp_job_run_op = train_model(\n",
    "        data_bucket = data_bucket\n",
    "    )\n",
    "    \n",
    "    compare_model_job_run_op = compare_model(\n",
    "        new_model = training_mlp_job_run_op.outputs[\"output_model\"],\n",
    "        model_bucket_metadata = model_bucket_metadata\n",
    "    )\n",
    "    \n",
    "    with dsl.If(compare_model_job_run_op.outputs[\"decision\"] == \"NEW\"):\n",
    "        upload_model_to_gcs(\n",
    "            project_id = project_id,\n",
    "            model_repo = model_repo\n",
    "        )\n",
    "    \"\"\"\n",
    "    # Old stuff\n",
    "    \n",
    "    di_op = download_data(\n",
    "        project_id=project_id,\n",
    "        bucket=data_bucket,\n",
    "        file_name=trainset_filename\n",
    "    )\n",
    "\n",
    " \n",
    "    training_mlp_job_run_op = train_mlp(\n",
    "        features=di_op.outputs[\"dataset\"]\n",
    "    )\n",
    "    \n",
    "     \n",
    "    training_lr_job_run_op = train_lr(\n",
    "        features=di_op.outputs[\"dataset\"]\n",
    "    )\n",
    "    \n",
    "    pre_di_op = download_data(\n",
    "        project_id=project_id,\n",
    "        bucket=data_bucket,\n",
    "        file_name=testset_filename\n",
    "    ).after(training_mlp_job_run_op, training_lr_job_run_op)\n",
    "        \n",
    "        \n",
    "    comp_model__op = compare_model(mlp_metrics=training_mlp_job_run_op.outputs[\"metrics\"],\n",
    "                                       lr_metrics=training_lr_job_run_op.outputs[\"metrics\"]).after(training_mlp_job_run_op, training_lr_job_run_op)  \n",
    "    \n",
    "    # defining the branching condition\n",
    "    with dsl.If(comp_model__op.output==\"MLP\"):\n",
    "        predict_mlp_job_run_op = predict_mlp(\n",
    "            model=training_mlp_job_run_op.outputs[\"out_model\"],      \n",
    "            features=pre_di_op.outputs[\"dataset\"]\n",
    "        )\n",
    "        upload_model_mlp_to_gc_op = upload_model_to_gcs(\n",
    "            project_id=project_id,\n",
    "            model_repo=model_repo,\n",
    "            model=training_mlp_job_run_op.outputs['out_model']\n",
    "        ).after(predict_mlp_job_run_op)\n",
    "        \n",
    "    with dsl.If(comp_model__op.output==\"LR\"):\n",
    "        predict_lr_job_run_op = predict_lr(\n",
    "            model=training_lr_job_run_op.outputs[\"out_model\"],     \n",
    "            features=pre_di_op.outputs[\"dataset\"]\n",
    "        )\n",
    "        upload_model_lr_to_gc_op = upload_model_to_gcs(\n",
    "            project_id=project_id,\n",
    "            model_repo=model_repo,\n",
    "            model=training_lr_job_run_op.outputs['out_model']\n",
    "        ).after(predict_lr_job_run_op) \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac278200-c580-4f40-bc8b-1817d3b13c13",
   "metadata": {},
   "source": [
    "#### Compile the pipeline into a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8ee4b21-89e6-4f63-845c-b249556ea919",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import compiler\n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='cloudbuild.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f87025e-08d7-4608-b37d-c929b6eb5a3c",
   "metadata": {},
   "source": [
    "#### Submit the pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b88e89-42cd-4e64-bc4e-8e3eddebccff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "# Before initializing, make sure to set the GOOGLE_APPLICATION_CREDENTIALS\n",
    "# environment variable to the path of your service account.\n",
    "aip.init(\n",
    "    project=PROJECT_ID,\n",
    "    staging_bucket=PIPELINE_ROOT,\n",
    ")\n",
    "\n",
    "# Prepare the pipeline job\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"banknote-authentication-training-pipeline\"\",\n",
    "    enable_caching=False,\n",
    "    template_path=\"cloudbuild.yaml\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    location=REGION,\n",
    "    parameter_values={\n",
    "        'project_id': PROJECT_ID, # makesure to use your project id \n",
    "        'data_bucket': 'ass1_data_bucket',  # makesure to use your data bucket name \n",
    "        'model_repo':'ass1_model_bucket' # makesure to use your model bucket name\n",
    "        'model_bucket_metadata': 'gs://ass1_model_bucket/model_metadata.json'\n",
    "    }\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m134",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m134"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
